---
title: "Práctica 1"
subtitle: "Grupo 4"
author: 
        - Iria Lago Portela
        
        - Mario Picáns Rey 
        
        - Javier Kniffki 
        
        - David Bamio Martínez
        
output: pdf_document
header-includes:
    - \renewcommand{\and}{\\}
---

# Ejercicios

En primer lugar vamos a cargar los datos y los paquetes necesarios para la realización de esta práctica:

```{r, warning=F, message=F, echo=F}
# Librerías 
ipak <- function(pkg){
        new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
        if (length(new.pkg)) 
                install.packages(new.pkg, dependencies = TRUE)
        sapply(pkg, require, character.only = TRUE)
}

packages <- c("rpart", "rpart.plot", "caret", "randomForest", "pdp", "kernlab")
ipak(packages)

# Datos
load("data/College4.RData")
head(College4)
dim(College4)
```
Este conjunto de datos está formado por 500 universidades públicas (Private=='No') y privadas (Private=='Yes') de EE.UU., para las cuales se observan 17 variables.

Para mejorar la interpretación de los resultados modificaremos la variable tipo de Universidad **Private**, de modo que 'Yes' sea Privada y 'No' sea Pública.

```{r}
datos <- College4[,-1]

datos$Tipo <- factor(College4$Private == "Yes", labels = c("Pública", "Privada")) 

head(datos)

```

Además, nótese que g.

```{r}
#Proporción privada-pública
table(datos$Tipo)

```

## 1.  Obtener un árbol de decisión que permita clasificar las observaciones (universidades) 
    en privadas (`Private="Yes"`) o públicas (`Private="No"`).

### a. Seleccionar el parámetro de complejidad de forma automática, siguiendo el criterio de un error estándar de Breiman et al. (1984).
  
En primer lugar vamos a considerar el 80% de las observaciones como muestra de entrenamiento y el 20% restante como muestra de test.

Establecemos la semilla igual al número de grupo multiplicado por 10, utilizando la función `set.seed` de R:

```{r}
#Semilla
set.seed(40)

nobs <- nrow(datos) #Filas
itrain <- sample(nobs, 0.8 * nobs) 
train <- datos[itrain, ] # M. Entrenamiento
test <- datos[-itrain, ] # M. Test

```

En primer lugar obtendremos un árbol que nos permita clasificar las universidades en privadas y públicas, utilizando la muestra de entrenamiento.
  
```{r}
tree<-rpart(Tipo~.,data=train)

rpart.plot(tree,main="Árbol de clasificación privada-pública")
```

El resultado es un árbol con 5 nodos terminales, por lo que puede ser interesante podarlo.

Para el proceso de poda seleccionaremos un paramétro de complejidad de forma automática, siguiendo el criterio de un error estándar de Breiman et al. (1984).

```{r}
tree <- rpart(Tipo ~ ., data = train, cp = 0)
plotcp(tree)


xerror <- tree$cptable[,"xerror"]
imin.xerror <- which.min(xerror)
upper.xerror <- xerror[imin.xerror] + tree$cptable[imin.xerror, "xstd"]
icp <- min(which(xerror <= upper.xerror))
cp <- tree$cptable[icp, "CP"]
cp
```

En primer lugar fijamos el parámetro $cp=0$, es decir, ajustamos el árbol completo. A continuación se calculan los errores de validación cruzada (reescalados) dependiendo del parámetro de complejidad empleado en el ajuste del árbol de decisión. Usando el criterio del error estándar de Breiman nos quedamos con el valor de cp que de lugar al mínimo error, en este caso $cp=0.02521008$.
   
    
### b. Representar e interpretar el árbol resultante.
  
Si podamos el árbol utilizando el valor del parámetro obtenido en el apartado anterior, obtenemos el siguiente árbol:

```{r}
tree<-prune(tree,cp=cp)

rpart.plot(tree, 
           extra = 104,          # show fitted class, probs, percentages
           box.palette = "GnBu", # color scheme
           branch.lty = 3,       # dotted branch lines
           shadow.col = "gray",  # shadows under the node boxes
           main="Árbol de clasificación privada-pública",
           nn = TRUE)
```
En este caso obtuvimos un árbol con 4 nodos terminales, que contienen un 24%, 8%, 6% y 62% del total de los datos respectivamente. 

El nodo inicial o nodo padre contiene el total de los datos, para los cuales el 70% de los datos son universidades privadas y el 30% son públicas. Dado que la moda o mayoría de universidades son privadas clasifica como privada.

A continuación el árbol se divide en dos ramas teniendo en cuenta la variable **Outstate**, es decir, el número de estudiantes de otro estado (en miles). Si el número de estudiantes de otro estado es menor que 8000 entonces clasificará como universidad pública, mientras que si es mayor clasificará como privada. 

En el nodo 2 se encuentra un 32% de los datos, para los cuales el 75% son universidades públicas y el 25% privadas. 

En el nodo 3 se encuentra un 68% de los datos, para los cuales el 9% de los datos son universidades públicas y el 91% son privadas. 

A continuación el nodo 2 se divide en otras dos ramas teniendo en cuenta la variable **Accept**, es decir, el número de solicitudes aceptadas en escala logarítmica. Si el número de solicitudes aceptadas es mayor o igual que 6.7 entonces clasificará como universidad pública, mientras que si es menor clasificará como privada.

Por otra parte el nodo 3 se divide en dos teniendo en cuenta la variable **Enroll**, es decir, el número de nuevos estudiantes matriculados en escala logarítmica. Si el número de nuevos estudiantes es mayor o igual que 7.3, el árbol clasificará como universidad pública, mientras que si es menor clasificará como universidad privada.

En el primer nodo terminal se encuentra un 24% de los datos, de los cuales el 89% de las universidades son públicas y el 11% restante son privadas. Dado que hay un mayor número de universidades públicas clasifica en públicas. 

En el segundo nodo terminal se encuentra un 8% de los datos, de los cuales el 29% de las universidades son públicas y el 71% restante son privadas, por lo que clasifica en privadas.

En el tercer nodo terminal se encuentra un 6% de los datos, de los cuales el 77% de las universidades son públicas y el 23% restante son privadas, por lo que clasifica en públicas.

En el último nodo terminal se encuentra un 62% de los datos, de los cuales el 2% de las universidades son públicas y el 98% restante son privadas, por lo que clasifica en privadas.

Nótese que tanto el primer como el último nodo terminal poseen colores más oscuros, esto indica que en estos nodos la clasificación es mejor.

### c. Evaluar la precisión, de las predicciones y de las estimaciones de la probabilidad, en la muestra de test.
    
Por último nos piden evaluar la precisión de las predicciones y de las estimaciones de la probabilidad en la muestra de test. Para ello debemos obtener las observaciones de la muestra de test y compararlas con las predicciones obtenidas con nuestro modelo.
    
```{r}

obs <- test$Tipo # Observaciones
pred <- predict(tree, newdata = test, type = "class") #Predicciones

confusionMatrix(pred,obs)
```
En primer lugar obtenemos la matriz de confusión, donde enfrentamos observaciones frente a predicciones. En este caso hemos obtenido que el modelo clasifica bien 17 universidades públicas de un total de 24 y 70 universidades privadas de un total de 76. Luego nuestro modelo tiene una precisión de las predicciones de un 87%. 

Sin embargo, hay que tener en cuenta que se trata de una muestra desbalanceada, puesto que contiene 143 universidades públicas y 357 universidades privadas. En estos casos conviene fijarse en el Kappa, que posee un valor más bajo, del 63.85%.

Para calcular la precisión de las estimaciones de la probabilidad, debemos utilizar la función `pred`con la opción por defecto `type="prob":

```{r}
pred_prob <- predict(tree, newdata = test) #Estimaciones de la probabilidad
head(pred_prob)
```

Así obtenemos la probabilidad de que cada Universidad sea pública o privada. 





## 2.  Realizar la clasificación anterior empleando Bosques Aleatorios mediante 
    el método `"rf"` del paquete `caret`.

### a. Considerar 300 árboles y seleccionar el número de predictores empleados en cada división `mtry = c(1, 2, 4, 6)` mediante validación cruzada, con 10 grupos y empleando el criterio de un error  estándar de Breiman.
    
```{r}
tuneGrid <- data.frame(mtry = c(1, 2, 4, 6))

rf.caret <-
    train(
        Tipo ~ .,
        data = train,
        method = "rf",
        ntree = 300,
        tuneGrid = tuneGrid,
        trControl = trainControl(
            method = "cv",
            number = 10,
            selectionFunction = "oneSE"
        )
    )

final <- rf.caret$finalModel

```

### b. Representar la convergencia del error en las muestras OOB en el modelo final.
    
```{r}
plot(final, main = "Tasas de error OOB")
legend("topright",
       colnames(final$err.rate),
       lty = 1:5,
       col = 1:6)
```
    
    
### c. Estudiar la importancia de las variables y el efecto de las principales empleando algún método gráfico (para la interpretación del modelo).
    
```{r}
importance(final)
varImpPlot(final)

pdp1 <- partial(final, "Outstate", train = train)
p1 <- plotPartial(pdp1)

pdp2 <- partial(final, "Enroll", train = train)
p2 <- plotPartial(pdp2)
grid.arrange(p1, p2, ncol = 2)
```
    

### d. Evaluar la precisión de las predicciones en la muestra de test y comparar los resultados con los obtenidos con el modelo del ejercicio anterior.
    
```{r}
obs <- test$Tipo
head(predict(final, newdata = test))
pred <- predict(final, newdata = test, type = "class")
table(obs, pred)

confusionMatrix(pred, obs)
```
    


## 3.  Realizar la clasificación anterior empleando SVM mediante la función `ksvm()` del paquete `kernlab`,

### a. Ajustar el modelo con las opciones por defecto.
    
```{r}
set.seed(40)
svm <- ksvm(Tipo ~ ., data = train)
svm

pred <- predict(svm, newdata = test)
confusionMatrix(pred, test$Tipo)

```
    
    
### b. Ajustar el modelo empleando validación cruzada con 10 grupos para seleccionar los valores "óptimos" de los hiperparámetros, considerando las posibles combinaciones de `sigma = c(0.01, 0.05, 0.1)` y `C = c(0.5, 1, 10)` (sin emplear el  paquete `caret`; ver Ejercicio 3.1 en *03-bagging_boosting-ejercicios.html*).
    
```{r}
tune.grid <- expand.grid(
    sigma = c(0.01, 0.05, 0.1),
    C = c(0.5, 1, 10),
    error = NA
)

best.err <- Inf
set.seed(40)
for (i in 1:nrow(tune.grid)) {
    fit <-
        ksvm(
            Tipo ~ .,
            data = train[, ],
            cross = 10,
            C = tune.grid$C[i],
            kpar = list(tune.grid$sigma[i])
        )
    fit.error <- fit@cross
    tune.grid$error[i] <- fit.error
    if (fit.error < best.err) {
        final.model <- fit
        best.err <- fit.error
        best.tune <- tune.grid[i,]
    }
}

final.model

pred2 <- predict(final.model, newdata = test)
confusionMatrix(pred2, test$Tipo)

```
    
     
### c. Evaluar la precisión de las predicciones de ambos modelos en la muestra de test y comparar también los resultados con los obtenidos en el ejercicio anterior. 
     
