---
title: "Práctica 1"
subtitle: "Grupo 4"
author: 
        - Iria Lago Portela
        
        - Mario Picáns Rey 
        
        - Javier Kniffki 
        
        - David Bamio Martínez
        
output: pdf_document
header-includes:
    - \renewcommand{\and}{\\}
---

# Ejercicios

En primer lugar vamos a cargar los datos y los paquetes necesarios para la realización de esta práctica:

```{r, warning=F, message=F, echo=F}
# Librerías 
ipak <- function(pkg){
        new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
        if (length(new.pkg)) 
                install.packages(new.pkg, dependencies = TRUE)
        sapply(pkg, require, character.only = TRUE)}
```
```{r, warning=F, message=F,results='hide'}
# Librerías 
packages <- c("rpart", "rpart.plot", "caret", "randomForest", "pdp", "kernlab")
ipak(packages)
```
```{r, warning=F, message=F}
# Datos
load("data/College4.RData")
head(College4, n = 3)
dim(College4)
```

Para mejorar la interpretación de los resultados modificaremos la variable tipo de Universidad **Private**, de modo que 'Yes' sea Privada y 'No' sea Pública.

```{r}
datos <- College4[,-1]

datos$Tipo <- factor(College4$Private == "Yes", labels = c("Pública", "Privada")) 

head(datos,n = 3)

```

Además, nótese que:

```{r}
#Proporción privada-pública
table(datos$Tipo)

```

## 1.  Obtener un árbol de decisión que permita clasificar las observaciones (universidades) en privadas (`Private = "Yes"`) o públicas (`Private = "No"`).

### a. Seleccionar el parámetro de complejidad de forma automática, siguiendo el criterio de un error estándar de Breiman et al. (1984).
  
En primer lugar vamos a considerar el 80% de las observaciones como muestra de entrenamiento y el 20% restante como muestra de test.

Establecemos la semilla igual al número de grupo multiplicado por 10, utilizando la función `set.seed` de R:

```{r}
#Semilla
set.seed(40)

nobs <- nrow(datos) #Filas
itrain <- sample(nobs, 0.8 * nobs) 
train <- datos[itrain, ] # M. Entrenamiento
test <- datos[-itrain, ] # M. Test

```

En primer lugar obtendremos un árbol que nos permita clasificar las universidades en privadas y públicas, utilizando la muestra de entrenamiento.
  
```{r,out.width="60%"}
tree<-rpart(Tipo~.,data=train)

rpart.plot(tree,main="Árbol de clasificación privada-pública")
```

El resultado es un árbol con 5 nodos terminales, por lo que puede ser interesante podarlo.

Para el proceso de poda seleccionaremos un paramétro de complejidad de forma automática, siguiendo el criterio de un error estándar de Breiman et al. (1984).

```{r,out.width="60%"}
tree <- rpart(Tipo ~ ., data = train, cp = 0)
plotcp(tree)


xerror <- tree$cptable[,"xerror"]
imin.xerror <- which.min(xerror)
upper.xerror <- xerror[imin.xerror] + tree$cptable[imin.xerror, "xstd"]
icp <- min(which(xerror <= upper.xerror))
cp <- tree$cptable[icp, "CP"]
cp
```

En primer lugar fijamos el parámetro $cp=0$, es decir, ajustamos el árbol completo. A continuación se calculan los errores de validación cruzada (reescalados) dependiendo del parámetro de complejidad empleado en el ajuste del árbol de decisión. Usando el criterio del error estándar de Breiman nos quedamos con el valor de cp que de lugar al mínimo error, en este caso $cp=0.02521008$.
   
    
### b. Representar e interpretar el árbol resultante.
  
Si podamos el árbol utilizando el valor del parámetro obtenido en el apartado anterior, obtenemos el siguiente árbol:

```{r,out.width="60%"}
tree<-prune(tree,cp=cp)

rpart.plot(tree, 
           extra = 104,          # show fitted class, probs, percentages
           box.palette = "GnBu", # color scheme
           branch.lty = 3,       # dotted branch lines
           shadow.col = "gray",  # shadows under the node boxes
           main="Árbol de clasificación privada-pública",
           nn = TRUE)
```

En este caso obtuvimos un árbol con 4 nodos terminales, que contienen un 24%, 8%, 6% y 62% del total de los datos respectivamente. 

El nodo inicial o nodo padre contiene el total de los datos, para los cuales el 70% de los datos son universidades privadas y el 30% son públicas. Dado que la moda o mayoría de universidades son privadas, clasifica como privada.

A continuación el árbol se divide en dos ramas teniendo en cuenta la variable **Outstate**, es decir, el número de estudiantes de otro estado (en miles). Si el número de estudiantes de otro estado es menor que 8000 entonces clasificará como universidad pública, mientras que si es mayor clasificará como privada. 

En el nodo 2 se encuentra un 32% de los datos, para los cuales el 75% son universidades públicas y el 25% privadas. 

En el nodo 3 se encuentra un 68% de los datos, para los cuales el 9% de los datos son universidades públicas y el 91% son privadas. 

A continuación el nodo 2 se divide en otras dos ramas teniendo en cuenta la variable **Accept**, es decir, el número de solicitudes aceptadas en escala logarítmica. Si el número de solicitudes aceptadas es mayor o igual que 6.7 entonces clasificará como universidad pública, mientras que si es menor clasificará como privada.

Por otra parte el nodo 3 se divide en dos teniendo en cuenta la variable **Enroll**, es decir, el número de nuevos estudiantes matriculados en escala logarítmica. Si el número de nuevos estudiantes es mayor o igual que 7.3, el árbol clasificará como universidad pública, mientras que si es menor clasificará como universidad privada.

En el primer nodo terminal se encuentra un 24% de los datos, de los cuales el 89% de las universidades son públicas y el 11% restante son privadas. Dado que hay un mayor número de universidades públicas clasifica en públicas. 

En el segundo nodo terminal se encuentra un 8% de los datos, de los cuales el 29% de las universidades son públicas y el 71% restante son privadas, por lo que clasifica en privadas.

En el tercer nodo terminal se encuentra un 6% de los datos, de los cuales el 77% de las universidades son públicas y el 23% restante son privadas, por lo que clasifica en públicas.

En el último nodo terminal se encuentra un 62% de los datos, de los cuales el 2% de las universidades son públicas y el 98% restante son privadas, por lo que clasifica en privadas.

Nótese que tanto el primer como el último nodo terminal poseen colores más oscuros, esto indica que en estos nodos la clasificación es mejor.

### c. Evaluar la precisión, de las predicciones y de las estimaciones de la probabilidad, en la muestra de test.
    
Por último, nos piden evaluar la precisión de las predicciones y de las estimaciones de la probabilidad en la muestra de test. Para ello debemos obtener las observaciones de la muestra de test y compararlas con las predicciones obtenidas con nuestro modelo.
    
```{r}

obs <- test$Tipo # Observaciones
pred <- predict(tree, newdata = test, type = "class") #Predicciones

confusionMatrix(pred,obs)
```
En primer lugar obtenemos la matriz de confusión, donde enfrentamos observaciones frente a predicciones. En este caso hemos obtenido que el modelo clasifica bien 17 universidades públicas de un total de 24 y 70 universidades privadas de un total de 76. Luego nuestro modelo tiene una precisión de las predicciones de un 87%. 

Sin embargo, hay que tener en cuenta que se trata de una muestra desbalanceada, puesto que contiene 143 universidades públicas y 357 universidades privadas. En estos casos conviene fijarse en el valor de Kappa, que posee un valor más bajo, del 63.85%.

Para calcular la precisión de las estimaciones de la probabilidad, debemos utilizar la función `pred` con la opción por defecto `type="prob"`:

```{r}
pred_prob <- predict(tree, newdata = test) #Estimaciones de la probabilidad
head(pred_prob)
```

Así obtenemos las probabilidades estimadas de que cada Universidad sea pública o privada. 





## 2.  Realizar la clasificación anterior empleando Bosques Aleatorios mediante el método `"rf"` del paquete `caret`.

### a. Considerar 300 árboles y seleccionar el número de predictores empleados en cada división `mtry = c(1, 2, 4, 6)` mediante validación cruzada, con 10 grupos y empleando el criterio de un error  estándar de Breiman.
 
Para seleccionar el valor de `mtry` mediante validación cruzada, se crea en primer lugar una rejilla con los valores que se van a considerar. El argumento `trControl`  de la función `train` permite seleccionar el método de validación cruzada (`method="cv"`) junto con el criterio de un error estandar (`selectionFunction="oneSE"`).

```{r}
set.seed(40)

tuneGrid <- data.frame(mtry = c(1, 2, 4, 6))

rf.caret <-
    train(
        Tipo ~ .,
        data = train,
        method = "rf",
        ntree = 300,
        tuneGrid = tuneGrid,
        trControl = trainControl(
            method = "cv",
            number = 10,
            selectionFunction = "oneSE"
        )
    )

final <- rf.caret$finalModel;final

```

El criterio de validación cruzada ha seleccionado como mejor modelo el que considera `mtry=1`. Los errores de clasificación marginales son del 15.1% para las universidades públicas y del 2.1% para las privadas. Esta diferencia posiblemente se deba al hecho de que la muestra está desbalanceada.

### b. Representar la convergencia del error en las muestras OOB en el modelo final.
    
```{r,out.width="60%"}
plot(final, main = "Tasas de error OOB")
legend("topright",
       colnames(final$err.rate),
       lty = 1:5,
       col = 1:6)
```
    
No parece haber problemas de convergencia del error en las muestras OOB para el modelo con 300 árboles. Esta tasa de error se estabiliza en torno al 6%.
    
### c. Estudiar la importancia de las variables y el efecto de las principales empleando algún método gráfico (para la interpretación del modelo).

Se obtiene la importancia de las variables predictoras mediante la función `importance()` de `randomForest`. También se pueden representar gráficamente con `varImpPlot()`.
    
```{r,out.width="60%"}
importance(final)
varImpPlot(final)
```
 
La variable predictora más importante, con cierta diferencia sobre las demás, es **Outstate**. Le siguen **Enroll** y **P.Undergrad**. Para estudiar el efecto de estas variables predictoras en la respuesta, se pueden generar mediante el paquete `pdp` unos gráficos PDP que estiman los efectos individuales de los predictores. Se muestran a continuación los gráficos para las dos variables con mayor importancia, **Outstate** y **Enroll**:
 
```{r,out.width="60%"}
pdp1 <- partial(final, "Outstate", train = train)
p1 <- plotPartial(pdp1)

pdp2 <- partial(final, "Enroll", train = train)
p2 <- plotPartial(pdp2)
grid.arrange(p1, p2, ncol = 2)
```

Se observa en este gráfico que la variable **Outstate** se relaciona con la respuesta de manera inversa a **Enroll**. Como ya vimos en los árboles de clasificación del primer ejercicio, valores altos de **Outstate** (es decir, un número elevado de alumnos de otros estados) se relacionan con las universidades privadas, mientras que los valores altos de **Enroll** (es decir, un número elevado de nuevas admisiones) se relacionan más con las universidades públicas.

### d. Evaluar la precisión de las predicciones en la muestra de test y comparar los resultados con los obtenidos con el modelo del ejercicio anterior.
    
```{r}
obs <- test$Tipo
head(predict(final, newdata = test))
pred <- predict(final, newdata = test, type = "class")
table(obs, pred)

confusionMatrix(pred, obs)
```

La matriz de confusión del nuevo modelo indica que se clasifican correctamente 18 de las 24 universidades públicas y 72 de las 76 universidades privadas. Esto supone una cierta mejora con respecto al modelo del ejercicio 1, como refleja el aumento en la precisión (90% frente al 87% anterior) así como en el valor de Kappa (71.7% frente al 63.8% anterior).


## 3.  Realizar la clasificación anterior empleando SVM mediante la función `ksvm()` del paquete `kernlab`,

### a. Ajustar el modelo con las opciones por defecto.
    
En primer lugar, vamos a ajustar el modelo con los ajustes por defecto que trae la función `ksvm` del paquete `kernlab`, es decir, indicarle la fórmula (`Tipo~.`), y los datos de entrenamiento que se emplean para la clasificación (`train`).
    
```{r}
set.seed(40)
svm <- ksvm(Tipo ~ ., data = train,prob.model=TRUE)
svm

dim(train)
```

Observamos que por defecto emplea el parámetro de coste `C = 1`. Calcula $\hat{\sigma} = 0.054$, que es la inversa de la ventana; y emplea 114 vectores soporte, que es aprox. el $50\%$ de los datos.

Podemos realizar predicciones con la función `predict` del paquete base de R. Con el modelo ajustado `svm`, y con el conjunto de datos de test obtenemos las predicciones.
    
```{r}
pred <- predict(svm, newdata = test)
```
    
En el apartado c) analizaremos la precisión de las predicciones del modelo ajustado en este apartado.
    
### b. Ajustar el modelo empleando validación cruzada con 10 grupos para seleccionar los valores "óptimos" de los hiperparámetros, considerando las posibles combinaciones de `sigma = c(0.01, 0.05, 0.1)` y `C = c(0.5, 1, 10)` (sin emplear el  paquete `caret`; ver Ejercicio 3.1 en *03-bagging_boosting-ejercicios.html*).

En primer lugar, creamos el grid con todas las combinaciones posibles de los hiperparámetros.    

```{r}
tune.grid <- expand.grid(
    sigma = c(0.01, 0.05, 0.1),
    C = c(0.5, 1, 10),
    error = NA
)
```

A continuación, como se realiza de forma análoga en *03-bagging_boosting-ejercicios.html*, ajustamos un modelo con cada combinación de hiperparámetros, comprobando en cada iteración si hay una mejora en el modelo.
    
```{r}
best.err <- Inf
set.seed(40)
for (i in 1:nrow(tune.grid)) {
    fit <-
        ksvm(
            Tipo ~ .,
            prob.model=TRUE,
            data = train[, ],
            cross = 10,
            C = tune.grid$C[i],
            kpar = list(tune.grid$sigma[i])
        )
    fit.error <- fit@cross
    tune.grid$error[i] <- fit.error
    if (fit.error < best.err) {
        final.model <- fit
        best.err <- fit.error
        best.tune <- tune.grid[i,]
    }
}
```
    
Además de obtener el ajuste que se requería, obtenemos los errores para cada combinación (almacenados en `tune.grid`). El ajuste elegido es el que genera un menor error, y para ver con qué combinación de hiperparámetros se ajusta el modelo podemos consultar `best.tune`:
```{r}
best.tune
```

Observamos que el mejor modelo es el que emplea $\sigma = 0.01$, y un parámetro de coste $C = 0.5$. Podemos comprobar el ajuste:
  
```{r}
final.model
```
    
De la misma forma que en el apartado a), podemos realizar predicciones con la función `predict`, el ajuste `final.model`, y el conjunto de datos de test.
    
```{r}
pred2 <- predict(final.model, newdata = test)
```
    
En el próximo apartado mediremos tanto su capacidad predictiva, como la del ajuste calculado en el apartado anterior. Además, compararemos los modelos obtenidos en este ejercicio, con los del ejercicio 2.
     
### c. Evaluar la precisión de las predicciones de ambos modelos en la muestra de test y comparar también los resultados con los obtenidos en el ejercicio anterior. 
En primer lugar, evaluaremos la precisión del modelo obtenido las matrices de confusión de las dos predicciones. Lo realizamos con la función `confusionMatrix` del paquete `caret`:

```{r}
caret::confusionMatrix(pred, test$Tipo)
```


```{r}
caret::confusionMatrix(pred2,test$Tipo)
```

